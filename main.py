#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
from dotenv import load_dotenv

# ==================== 0. ç¯å¢ƒåˆå§‹åŒ– ====================
# è‡ªåŠ¨åŠ è½½ç¯å¢ƒå˜é‡ (å…¼å®¹æ¨¡å¼ï¼šæœ‰æ–‡ä»¶è¯»æ–‡ä»¶ï¼Œæ²¡æ–‡ä»¶è¯» Secrets)
load_dotenv()

# è°ƒè¯•æ‰“å°ï¼šæ£€æŸ¥å¯†é’¥æ˜¯å¦å­˜åœ¨ (ä¸ä¼šæ‰“å°çœŸå®å€¼)
print("-" * 30)
print("ğŸ” ç¯å¢ƒè‡ªæ£€...")
if os.getenv('GITHUB_ACTIONS'):
    print("â˜ï¸ è¿è¡Œç¯å¢ƒ: GitHub Actions (è‡ªåŠ¨ç›´è¿æ¨¡å¼)")
else:
    print("ğŸŒ è¿è¡Œç¯å¢ƒ: æœ¬åœ° (å°è¯•åŠ è½½ä»£ç†)")
    # åªæœ‰åœ¨æœ¬åœ°æ‰å¼ºåˆ¶èµ°ä»£ç†ï¼ŒGitHub ä¸Šç»å¯¹ä¸èƒ½è®¾
    os.environ['HTTP_PROXY'] = 'http://127.0.0.1:21879'
    os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:21879'

if os.getenv('GOOGLE_API_KEY'):
    print("âœ… GOOGLE_API_KEY: å·²æ£€æµ‹åˆ°")
else:
    print("âŒ é”™è¯¯: æœªæ‰¾åˆ° GOOGLE_API_KEYï¼Œè¯·æ£€æŸ¥ GitHub Settings -> Secrets")
    
if os.getenv('PUSHPLUS_TOKEN'):
    print("âœ… PUSHPLUS_TOKEN: å·²æ£€æµ‹åˆ°")
else:
    print("âŒ é”™è¯¯: æœªæ‰¾åˆ° PUSHPLUS_TOKENï¼Œè¯·æ£€æŸ¥ GitHub Settings -> Secrets")
print("-" * 30)

# ==================== å¸¸é‡é…ç½® ====================
TARGET_URL = "https://news.aibase.com/zh/daily"
PUSHPLUS_URL = "http://www.pushplus.plus/send"
# ä¼ªè£…æˆæµè§ˆå™¨ï¼Œé˜²æ­¢ 403 Forbidden
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# ==================== 1. çˆ¬è™«æ¨¡å— ====================
def scrape_news():
    print(f"ğŸ•·ï¸ å¼€å§‹æŠ“å–: {TARGET_URL}")
    try:
        # æ³¨æ„ï¼šè¿™é‡Œåƒä¸‡ä¸èƒ½ä¼  proxies å‚æ•°ï¼Œè®© requests è‡ªåŠ¨å¤„ç†
        resp = requests.get(TARGET_URL, headers=HEADERS, timeout=30)
        resp.raise_for_status() # å¦‚æœ 404 æˆ– 403 ä¼šç›´æ¥æŠ¥é”™
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        news_list = []
        
        # é’ˆå¯¹ AIbase çš„é€šç”¨æŠ“å–ç­–ç•¥
        # æŸ¥æ‰¾æ‰€æœ‰çš„é“¾æ¥ï¼Œç­›é€‰çœ‹èµ·æ¥åƒæ–°é—»çš„
        links = soup.find_all('a', href=True)
        
        for link in links:
            text = link.get_text(strip=True)
            href = link['href']
            
            # ç®€å•çš„ç­›é€‰é€»è¾‘ï¼šé•¿åº¦å¤Ÿé•¿ï¼Œä¸”åŒ…å« AI å…³é”®è¯
            if len(text) > 10 and any(k in text.lower() for k in ['ai', 'gpt', 'æ¨¡å‹', 'æ™ºèƒ½', 'daily', 'news']):
                if not href.startswith('http'):
                    href = f"https://news.aibase.com{href}"
                
                # å»é‡
                if not any(n['link'] == href for n in news_list):
                    news_list.append({'title': text, 'link': href})
            
            if len(news_list) >= 10: # åªè¦å‰10æ¡
                break
        
        if not news_list:
            print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–°é—»ï¼Œå°è¯•æŠ“å–é¡µé¢æ‰€æœ‰ H3 æ ‡é¢˜...")
            for h3 in soup.find_all('h3'):
                t = h3.get_text(strip=True)
                if len(t) > 5:
                    news_list.append({'title': t, 'link': TARGET_URL})
        
        print(f"âœ… æŠ“å–åˆ° {len(news_list)} æ¡å†…å®¹")
        return news_list

    except Exception as e:
        print(f"âŒ çˆ¬è™«å‡ºé”™: {e}")
        return []

# ==================== 2. AI æ€»ç»“æ¨¡å— ====================
def summarize(news_data):
    if not news_data:
        return "ä»Šæ—¥æš‚æ—  AI èµ„è®¯è·å–ã€‚"
        
    print("ğŸ§  æ­£åœ¨è¯·æ±‚ Gemini è¿›è¡Œæ€»ç»“...")
    try:
        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        # æ„å»º Prompt
        content_text = "\n".join([f"- {n['title']} ({n['link']})" for n in news_data])
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€ä¸»ç¼–ã€‚è¯·æ ¹æ®ä»¥ä¸‹ AI æ–°é—»åˆ—è¡¨ï¼Œå†™ä¸€ä»½ã€AI æ¯æ—¥æ—©æŠ¥ã€‘ã€‚
        
        è¦æ±‚ï¼š
        1. æŒ‘é€‰ 5-6 æ¡æœ€é‡è¦çš„å†…å®¹ã€‚
        2. æ¯æ¡ç”¨ Emoji å¼€å¤´ï¼Œä¸€å¥è¯æ¦‚æ‹¬ï¼Œå¹¶é™„å¸¦é“¾æ¥ã€‚
        3. åªè¦æ­£æ–‡ï¼Œä¸è¦åºŸè¯ã€‚
        
        æ–°é—»åˆ—è¡¨ï¼š
        {content_text}
        """
        
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"âŒ AI æ€»ç»“å‡ºé”™: {e}")
        return "AI æ€»ç»“æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"

# ==================== 3. æ¨é€æ¨¡å— ====================
def push_msg(content):
    print("ğŸ“¨ æ­£åœ¨æ¨é€...")
    token = os.getenv('PUSHPLUS_TOKEN')
    if not token:
        print("âŒ æ— æ³•æ¨é€ï¼šç¼ºå°‘ Token")
        return

    try:
        data = {
            'token': token,
            'title': 'AI æ¯æ—¥æ—©æŠ¥',
            'content': content,
            'template': 'markdown'
        }
        # æ³¨æ„ï¼šè¿™é‡Œä¹Ÿä¸èƒ½ä¼  proxies
        resp = requests.post(PUSHPLUS_URL, json=data, timeout=30)
        print(f"âœ… æ¨é€å“åº”: {resp.text}")
    except Exception as e:
        print(f"âŒ æ¨é€å‡ºé”™: {e}")

# ==================== ä¸»å…¥å£ ====================
if __name__ == "__main__":
    # 1. æŠ“å–
    news = scrape_news()
    
    # 2. å¦‚æœæ²¡æŠ“åˆ°ï¼Œä¸æŠ¥é”™é€€å‡ºï¼Œè€Œæ˜¯å‘é€ä¸€æ¡â€œç©ºæ—¥æŠ¥â€æé†’ï¼Œæ–¹ä¾¿æ’æŸ¥
    if not news:
        final_content = "âš ï¸ è­¦å‘Šï¼šçˆ¬è™«ä»Šæ—¥æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç«™ç»“æ„æ˜¯å¦å˜æ›´ã€‚"
    else:
        # 3. æ€»ç»“
        final_content = summarize(news)
    
    # 4. å‘é€
    push_msg(final_content)
    print("ğŸš€ ä»»åŠ¡å…¨éƒ¨å®Œæˆ")
