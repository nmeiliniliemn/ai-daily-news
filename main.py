#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI æ¯æ—¥æ—©æŠ¥ 2.0
è‡ªåŠ¨ä» AIbase è·å–æœ€æ–° AI æ–°é—»ï¼Œä½¿ç”¨ Google Gemini æ€»ç»“ï¼Œå¹¶æ¨é€è‡³ PushPlus
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
from dotenv import load_dotenv

# ==================== ç¯å¢ƒä¸ç½‘ç»œé…ç½® ====================
# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('config.env')  # ä½¿ç”¨ config.env æ–‡ä»¶è€Œä¸æ˜¯ .env

# å¼ºåˆ¶è®¾ç½®ä»£ç†ï¼ˆéå¸¸é‡è¦ï¼‰
os.environ['HTTP_PROXY'] = 'http://127.0.0.1:21879'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:21879'
os.environ['http_proxy'] = 'http://127.0.0.1:21879'
os.environ['https_proxy'] = 'http://127.0.0.1:21879'

# ==================== å¸¸é‡é…ç½® ====================
TARGET_URL = "https://news.aibase.com/zh/daily"
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
PUSHPLUS_URL = "http://www.pushplus.plus/send"

# AI Prompt æ¨¡æ¿
AI_PROMPT = """ä½ æ˜¯ä¸€ä¸ª AI ç§‘æŠ€è§‚å¯Ÿå‘˜ã€‚è¯·æ ¹æ®æˆ‘æä¾›çš„ AIbase æœ€æ–°èµ„è®¯åˆ—è¡¨ï¼Œå†™ä¸€ä»½ã€AI æ¯æ—¥æ—©æŠ¥ã€‘ã€‚

è¦æ±‚ï¼š
æç‚¼ 5-8 æ¡æœ€é‡è¦çš„ AI è¡Œä¸šåŠ¨æ€ã€‚
æ¯ä¸€æ¡ç”¨ Emoji å¼€å¤´ï¼ˆå¦‚ ğŸ¤–, ğŸš€ï¼‰ï¼Œä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒï¼Œå¹¶åœ¨æœ«å°¾é™„ä¸ŠåŸæ–‡é“¾æ¥ã€‚
é£æ ¼ç®€æ´ã€é«˜ä¿¡å™ªæ¯”ã€‚
åªè¦è¾“å‡ºæœ€ç»ˆå†…å®¹ï¼Œä¸è¦å¤šä½™çš„åºŸè¯ã€‚

èµ„è®¯åˆ—è¡¨ï¼š
{news_content}
"""

# ==================== çˆ¬è™«æ¨¡å— ====================
def scrape_aibase_news():
    """
    ä» AIbase ç½‘ç«™çˆ¬å–æœ€æ–°çš„ AI æ–°é—»
    """
    try:
        print("ğŸ” å¼€å§‹çˆ¬å– AIbase æ–°é—»...")

        headers = {
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

        # è®¾ç½®ä»£ç†
        proxies = {
            'http': 'http://127.0.0.1:21879',
            'https': 'http://127.0.0.1:21879'
        }

        response = requests.get(TARGET_URL, headers=headers, proxies=proxies, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'

        soup = BeautifulSoup(response.text, 'html.parser')

        # æå–æ–°é—»åˆ—è¡¨
        news_items = []

        # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«æ–°é—»é“¾æ¥çš„å®¹å™¨
        news_containers = soup.find_all(['div', 'article', 'section'], class_=lambda x: x and any(keyword in x.lower() for keyword in ['news', 'article', 'item', 'card', 'post']))

        if not news_containers:
            # æ–¹æ³•2: æŸ¥æ‰¾æ‰€æœ‰ h3 å’Œ a æ ‡ç­¾
            news_containers = soup.find_all(['h3', 'a'])

        for container in news_containers[:20]:  # é™åˆ¶æœç´¢èŒƒå›´
            # æŸ¥æ‰¾æ ‡é¢˜å’Œé“¾æ¥
            title_elem = None
            link_elem = None

            # å¦‚æœæ˜¯ h3 æ ‡ç­¾ï¼Œç›´æ¥ä½¿ç”¨
            if container.name == 'h3':
                title_elem = container
                link_elem = container.find_parent('a') or container.find('a')

            # å¦‚æœæ˜¯ a æ ‡ç­¾
            elif container.name == 'a':
                title_elem = container
                link_elem = container

            # å¦‚æœæ˜¯å…¶ä»–å®¹å™¨ï¼ŒæŸ¥æ‰¾å†…éƒ¨çš„æ ‡é¢˜å’Œé“¾æ¥
            else:
                title_elem = container.find(['h3', 'h4', 'h2', 'a'])
                link_elem = container.find('a') or title_elem if title_elem and title_elem.name == 'a' else container.find('a')

            if title_elem and link_elem:
                title = title_elem.get_text(strip=True)
                link = link_elem.get('href')

                # è¿‡æ»¤æœ‰æ•ˆçš„æ–°é—»æ ‡é¢˜
                if (title and len(title) > 10 and
                    any(keyword in title.lower() for keyword in ['ai', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'chatgpt', 'gpt', 'llm', 'å¤§æ¨¡å‹', 'ç§‘æŠ€', 'æŠ€æœ¯']) and
                    link and 'http' in link):

                    # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„ URL
                    if not link.startswith('http'):
                        link = f"https://news.aibase.com{link}" if link.startswith('/') else link

                    news_items.append({
                        'title': title,
                        'link': link
                    })

                    # é™åˆ¶æ•°é‡
                    if len(news_items) >= 15:
                        break

        # å¦‚æœæ²¡æ‰¾åˆ°è¶³å¤Ÿçš„æ–°é—»ï¼Œå°è¯•å¦ä¸€ç§æ–¹æ³•
        if len(news_items) < 5:
            print("âš ï¸  æ ‡å‡†æ–¹æ³•æœªæ‰¾åˆ°è¶³å¤Ÿæ–°é—»ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
            all_links = soup.find_all('a', href=True)
            for link in all_links[:50]:  # æ£€æŸ¥å‰50ä¸ªé“¾æ¥
                title = link.get_text(strip=True)
                href = link['href']

                if (title and len(title) > 20 and  # å¢åŠ æ ‡é¢˜é•¿åº¦è¦æ±‚
                    any(keyword in title.lower() for keyword in ['ai', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'chatgpt', 'gpt', 'æ—¥æŠ¥', 'æ–°é—»']) and
                    ('daily' in href or 'news' in href or 'article' in href)):

                    if not href.startswith('http'):
                        href = f"https://news.aibase.com{href}" if href.startswith('/') else href

                    news_items.append({
                        'title': title[:100] + '...' if len(title) > 100 else title,  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                        'link': href
                    })

                    if len(news_items) >= 15:
                        break

        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°è¶³å¤Ÿæ–°é—»ï¼Œå°è¯•è§£æé¡µé¢ä¸­çš„æ–‡ç« å†…å®¹
        if len(news_items) < 3:
            print("âš ï¸  å¤‡ç”¨æ–¹æ³•ä»æœªæ‰¾åˆ°è¶³å¤Ÿæ–°é—»ï¼Œå°è¯•è§£æé¡µé¢å†…å®¹...")
            # æŸ¥æ‰¾åŒ…å«æ–°é—»å†…å®¹çš„divæˆ–section
            content_areas = soup.find_all(['div', 'section', 'article'], class_=lambda x: x and any(word in ' '.join(x).lower() for word in ['content', 'news', 'article', 'post', 'entry']))

            for area in content_areas[:5]:  # åªæ£€æŸ¥å‰5ä¸ªå†…å®¹åŒºåŸŸ
                # åœ¨å†…å®¹åŒºåŸŸå†…æŸ¥æ‰¾æ®µè½æˆ–åˆ—è¡¨é¡¹
                paragraphs = area.find_all(['p', 'li', 'h3', 'h4'])
                for para in paragraphs:
                    text = para.get_text(strip=True)
                    if (text and len(text) > 30 and len(text) < 200 and  # åˆé€‚çš„æ®µè½é•¿åº¦
                        any(keyword in text.lower() for keyword in ['ai', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'chatgpt', 'gpt'])):

                        news_items.append({
                            'title': text[:150] + '...' if len(text) > 150 else text,
                            'link': TARGET_URL  # ä½¿ç”¨ä¸»é¡µé“¾æ¥
                        })

                        if len(news_items) >= 10:  # é™ä½è¦æ±‚åˆ°10æ¡
                            break
                if len(news_items) >= 10:
                    break

        print(f"âœ… æˆåŠŸè·å– {len(news_items)} æ¡æ–°é—»")
        return news_items[:15]  # æœ€å¤šè¿”å›15æ¡

    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {str(e)}")
        raise

# ==================== AI æ€»ç»“æ¨¡å— ====================
def summarize_with_ai(news_items):
    """
    ä½¿ç”¨ Google Gemini å¯¹æ–°é—»è¿›è¡Œæ€»ç»“
    """
    try:
        print("ğŸ¤– å¼€å§‹ AI æ€»ç»“...")

        # æ£€æŸ¥ API å¯†é’¥
        api_key = os.getenv('GOOGLE_API_KEY')
        if not api_key:
            raise ValueError("æœªæ‰¾åˆ° GOOGLE_API_KEY ç¯å¢ƒå˜é‡")

        # é…ç½® Gemini
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')

        # å‡†å¤‡æ–°é—»å†…å®¹
        news_content = ""
        for i, item in enumerate(news_items, 1):
            news_content += f"{i}. {item['title']}\n   é“¾æ¥: {item['link']}\n\n"

        # ç”Ÿæˆæ€»ç»“
        prompt = AI_PROMPT.format(news_content=news_content)
        response = model.generate_content(prompt)

        summary = response.text.strip()
        print("âœ… AI æ€»ç»“å®Œæˆ")
        return summary

    except Exception as e:
        error_msg = str(e)
        print(f"âŒ AI æ€»ç»“å¤±è´¥: {error_msg}")

        # å¦‚æœæ˜¯APIç›¸å…³é—®é¢˜ï¼Œç”Ÿæˆç®€å•çš„æ–‡æœ¬æ€»ç»“
        if ("quota" in error_msg.lower() or "429" in error_msg or
            "404" in error_msg or "not found" in error_msg.lower() or
            "unavailable" in error_msg.lower()):
            print("âš ï¸  API è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–‡æœ¬æ€»ç»“...")
            simple_summary = "# AI æ¯æ—¥æ—©æŠ¥\n\nç”±äº API é™åˆ¶ï¼Œä»¥ä¸‹æ˜¯ä»Šæ—¥ AI æ–°é—»æ‘˜è¦ï¼š\n\n"
            for i, item in enumerate(news_items[:8], 1):  # æœ€å¤šæ˜¾ç¤º8æ¡
                simple_summary += f"{i}. {item['title']}\nğŸ”— {item['link']}\n\n"
            simple_summary += "\n*æ³¨ï¼šAPI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œå»ºè®®ç¨åé‡è¯•æˆ–å‡çº§ä»˜è´¹è®¡åˆ’*"
            return simple_summary
        else:
            # å¯¹äºå…¶ä»–æœªçŸ¥é”™è¯¯ï¼Œä»ç„¶æŠ›å‡ºå¼‚å¸¸
            raise

# ==================== æ¨é€æ¨¡å— ====================
def send_push_notification(title, content):
    """
    å‘é€æ¨é€é€šçŸ¥åˆ° PushPlus
    """
    try:
        print("ğŸ“± å¼€å§‹æ¨é€é€šçŸ¥...")

        # æ£€æŸ¥æ¨é€ä»¤ç‰Œ
        token = os.getenv('PUSHPLUS_TOKEN')
        if not token:
            raise ValueError("æœªæ‰¾åˆ° PUSHPLUS_TOKEN ç¯å¢ƒå˜é‡")

        # å‡†å¤‡æ¨é€æ•°æ®
        data = {
            'token': token,
            'title': title,
            'content': content,
            'template': 'markdown'
        }

        # è®¾ç½®ä»£ç†
        proxies = {
            'http': 'http://127.0.0.1:21879',
            'https': 'http://127.0.0.1:21879'
        }

        # å‘é€æ¨é€
        response = requests.post(PUSHPLUS_URL, json=data, proxies=proxies, timeout=30)
        response.raise_for_status()

        result = response.json()
        if result.get('code') == 200:
            print("âœ… æ¨é€æˆåŠŸ")
        else:
            raise ValueError(f"æ¨é€å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")

    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {str(e)}")
        raise

# ==================== ä¸»å‡½æ•° ====================
def main():
    """
    ä¸»å‡½æ•°ï¼šä¸²è”æ•´ä¸ªæµç¨‹
    """
    try:
        print("ğŸš€ AI æ¯æ—¥æ—©æŠ¥ 2.0 å¼€å§‹æ‰§è¡Œ")
        print("=" * 50)

        # 1. çˆ¬å–æ–°é—»
        news_items = scrape_aibase_news()
        if not news_items:
            raise ValueError("æœªè·å–åˆ°ä»»ä½•æ–°é—»å†…å®¹")

        # 2. AI æ€»ç»“
        summary = summarize_with_ai(news_items)

        # 3. æ¨é€é€šçŸ¥
        send_push_notification("AIæ¯æ—¥æ—©æŠ¥", summary)

        print("=" * 50)
        print("ğŸ‰ AI æ¯æ—¥æ—©æŠ¥æ‰§è¡Œå®Œæˆï¼")

    except Exception as e:
        print(f"ğŸ’¥ æ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)

# ==================== ç¨‹åºå…¥å£ ====================
if __name__ == "__main__":
    main()